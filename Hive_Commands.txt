hive> create database di;
hive> create database if not exists di;
hive> describe database d1;
hive> create database if not exists d2 comment 'this is d2 databse';
hive> describe database extended d2;
hive> create database if not exists d3 with dbproperties('creator'='Deepak','date'='04042019');

Table:
Internal:
hive> create table if not exists table1 (cal1 string,col2 array<string>,col3 string,col4 int) row format delimited fields terminated by ',' collection items terminated by ':' lines terminated by '\n' stored as textfile;

hive> create table if not exists table2 (cal1 string,col2 array<string>,col3 string,col4 int) row format delimited fields terminated by ',' collection items terminated by ':' lines terminated by '\n' stored as textfile location '/usr/hive/deepak';


    > load data local inpath '/home/cloudera/Downloads/table1.txt' into table table1;
hive> load data local inpath '/home/cloudera/Downloads/table1.txt' overwrite into table table1;



hive> set hive.metastore.warehouse.dir;
hive.metastore.warehouse.dir=/user/hive/warehouse
hive> set hive.cli.print.header=true



External:

hive> create external table if not exists table3 (col1 string,col2 array<string>,col3 string ,col4 int) row format delimited fields terminated by ',' collection items terminated by ':' lines terminated by '\n' stored as textfile;

hive> load data local inpath '/home/cloudera/Downloads/table1.txt' into table table3;

hive> load data local inpath '/home/cloudera/Downloads/table1.txt' overwrite into table table3;


------
hive> create table if not exists employee(col1 int,col2 string,col3 string,col4 int,col5 int,col6 int,col7 string) row format delimited fields terminated by ',' lines terminated by '\n' stored as textfile;

hive> create table if not exists tab(col1 int,col2 string,col3 string) stored as textfile;
hive> insert into table tab select col1,col2,col3 from employee; 
hive> insert overwrite table tab select col1,col2,col3 from employee where col2='Developer';
hive> from employee insert into table tab_dev select col1,col2,col3 where col3='Developer' insert into table tab_man select col1,col2,col3 where col3='Mgr';


hive> create table if not exists tab_order (col1 string,col2 int) row format delimited fields terminated by ',' lines terminated by '\n' stored as textfile;
hive> load data local inpath '/home/cloudera/Downloads/order.txt' into table tab_order;

hive> select col2 from tab_order order by col2;
hive> select col2 from tab_order order by col2 limit 5;
hive> select col2 from tab_order sort by col2;
hive> select col2 from tab_order distribute by col2;
hive> select col2 from tab_order distribute by col2 sort by col2;
hive> select col2 from tab_order cluster by col2;

Explode and Lateral View:
------------------
hive> create table if not exists books(col1 string,col2 array<string>) row format delimited fields terminated by ',' collection items terminated by ':' lines terminated by '\n' stored as textfile;
hive> load data local inpath '/home/cloudera/Downloads/explode-new-data.txt' into table books;
hive> select col1,auth from books lateral view explode(col2) dummy as auth;
hive> select col1,auth from books lateral view explode(col2) dummy as auth;


Rank functions:
--------------

hive> create table if not exists ranking_tab (col1 string , col2 int ) row format delimited fields terminated by ',' lines terminated by '\n' stored as textfile;
hive> load data local inpath '/home/cloudera/Downloads/rank.txt' into table ranking_tab;
hive> select col1,col2,rank() over(order by col2 desc) from ranking_tab;
hive> select col1,col2,dense_rank() over(order by col2 desc) from ranking_tab;
hive> select col1,col2,row_number() over(order by col2 desc) from ranking_tab;
hive> select col1,col2,row_number() over(partition by col1 order by col2 desc) from ranking_tab;


Partition and Bucketing:
-----------------------

Static:
Temp table:
hive> create table if not exists dept(col1 int,col2 string,col3 string,col4 int) row format delimited fields terminated by ',' lines terminated by '\n' stored as textfile;
hive> load data local inpath '/home/cloudera/Downloads/Partitioning/dept.txt' into table dept;

Create parttion table :
hive> create table if not exists dept_partition (empno int,empname string , sal int) partitioned by (depart string) row format delimited fields terminated by ',' lines terminated by '\n' stored as textfile; 

Load data with insert :
hive> insert into table dept_partition partition (depart='HR') select col1,col3,col4 from dept where col2='HR';

Load data with load:
hive> load data local inpath '/home/cloudera/Downloads/Partitioning/act' into table dept_partition partition (depart='Account'); 

Dyanamic:
hive> set hive.exec.dynamic.partition=true;
hive> set hive.exec.dynamic.partition.mode=nonstrict

hive> create table if not exists dept_part_dyn (empno int,empname string,empsal int) partitioned by (department string) row format delimited fields terminated by ',' lines terminated by '\n' stored as textfile;
hive> insert into table dept_part_dyn partition (department) select col1,col3,col4,col2 from dept;

hive> show partitions dept_part_dyn;

Alter Partition:
---------------
hive> alter table dept_part_dyn drop partition (department='Sales');
hive> alter table dept_part_dyn add partition (department='Dev');
hive> load data local inpath '/home/cloudera/Downloads/alter_partition/dev' into table dept_part_dyn partition (department='Dev');

Link directory into partition:
------------------------------
1st way:

[cloudera@quickstart Downloads]$ hdfs dfs -mkdir /user/hive/warehouse/db1.db/dept_part_dyn/department=Finance
hive> alter table dept_part_dyn add partition (department='Finance');

2nd way:
[cloudera@quickstart Downloads]$ hdfs dfs -mkdir /user/hive/warehouse/db1.db/dept_part_dyn/department=Finance
hive> msck repair table dept_part_dyn;


Bucketing:
----------
hive> set hive.enforce.bucketing = true;

Temp table
hive> create table if not exists dept_loc (col1 int,col2 string,col3 string,col4 int,col5 string) row format delimited fields terminated by ',' lines terminated by '\n' stored as textfile;
hive> load data local inpath '/home/cloudera/Downloads/dept-loc.txt' into table dept_loc;

hive> create table if not exists dept_buck (empno int,empname string,empsal int,emploc string) partitioned by (department string) clustered by (emploc) into 4 buckets row format delimited fields terminated by ',' lines terminated by '\n' stored as textfile;
hive> insert into table dept_buck partition (department) select col1,col3,col4,col5,col2 from dept_loc;

Table Sampling:
---------------
hive> select * from dept_buck tablesample (bucket 1 out of 3 on emploc);
hive> select * from dept_buck tablesample (1 percent);


Map Joins:
--------------
1st way:
Add MAPJOIN in query
hive> select /* + MAPJOIN */ table1.id,table1.department,table1.code,table2.department from table1 join table2 on (table1.id=table2.id);


2nd way:
set these 2 property-
hive> set hive.auto.convert.join = true;
hive> set hive.mapjoin.smalltable.filesize;
hive> select table1.id,table1.department,table1.code,table2.department from table1 join table2 on (table1.id=table2.id);

Buckted Join:
------------
Set these property-
hive> set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
hive> set hive.optimize.bucketmapjoin=true;
hive> set hive.auto.convert.sortmerge.join=true;
hive> set hive.optimize.bucketmapjoin.sortedmerge=true;


Views:
------

hive> create view emp_view1 as select * from employee;
hive> create view emp_view2 as select col1,col2,col3 from employee;
hive> create view emp_view3 as select table1.id,table1.department,table1.code from table1 join table2 on (table1.id=table2.id);
hive> create view emp_view4 as select * from emp_view1;
hive> alter view emp_view1 as select col1 from employee;
hive> alter view emp_view4 rename to emp_view5;

Indexes:
--------
hive> show formatted indexes on dept;
hive> drop index i4 on dept;

Compact:

hive> create index i1 on table dept(col3) as 'COMPACT' with deferred rebuild;
hive> alter index i1 on dept rebuild;
hive> create index i2 on table dept(col3) as 'COMPACT' with deferred rebuild stored as textfile;
hive> creare index i3 on table dept(col3) as 'COMPACT' with deferred rebuild row format delimited fields terminated by '\n' stored as textfile;

BITMAP:
hive> create index i4 on table dept(col3) as 'BITMAP' with deferred rebuild;
hive> select avg(col4) as average from dept where col2='HR';

Skipping header and footer from the i/p file while loading:
----------------------------------------------------------

Pass table property for header -> skip.header.line.count
		    for tailer --> skip.footer.line.count
hive> create table if not exists table1(col1 string,col2 int) row format delimited fields terminated by ',' lines terminated by '\n' stored as textfile tblproperties("skip.header.line.count"="3");
hive> load data local inpath '/home/cloudera/Downloads/log1.txt' into table table1;

hive> create table if not exists table2(col1 string,col2 int) row format delimited fields terminated by ',' lines terminated by '\n' stored as textfile tblproperties("skip.footer.line.count"="3");


Immutable Table:
---------------
hive> create table if not exists table3 (col1 string,col2 int) row format delimited fields terminated by ',' lines terinated by '\n' stored as textfile tblproperties("immutable"=true);

Operation allowed on immutable tables:
1. load data command is allowed, you can load data any number of time with load data command.
2. Inset into is allowed only once first time,if yo have already inserted data using load data then insert into will not work.
3. insert overwrite is allowed.

Insert NULL where field is blank:
--------------------------------
hive> create table if not exists table1(col1 string,col2 string,col3 int) row format delimited fields terminated by ',' lines terminated by '\n' stored as textfile tblproperties("serialization.null.format"="");

ACID/Transactional feature(Insert,Update,Delete):
--------------------------
Prerequsite:
1. stored File format should be ORD.
2. Table shoudl be bucketed.
3. Set below properties:
	set hive.support.concurrency=true
	set hive.enforce.bucketting=true
	set hive.exec.dynamic.partition.mode=nonstrict
	set hive.compactor.initiator.on=true
	set hive.compactor.worker.threads=1
	set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;

set below property in : /etc/hive/conf/hive-site.xml
			<configuration>
				<property>
					<name>hive.in.test</name>
					<value>true</value>	
				</property>
			</configuration>

hive> create table if not exists table1(col1 string,col2 int) clustered by (col1) into 4 buckets stored as orc tblproperties("transactional"="true");

ORC file format table properties:
--------------------------------
1. orc.compress=(zlib/snappy/...)
2. orc.compress.size="Value"
3. orc.strip.size="value"
4. orc.row.index.stride="value"


Hive Configuration:
-------------------



Paralle Execution:
-----------------
Set this property to true.
hive> set hive.exec.parallel=true;

Run Hive query from bash sell:
-----------------------------
Running hive query from bash ->
[cloudera@quickstart Downloads]$ hive -e 'select * from db1.dept;'


Running hive script:
1st from hive shell -->
hive> source /home/cloudera/Desktop/Deepak/hive_script.hql;

2nd from bash -->
[cloudera@quickstart Deepak]$ hive -f /home/cloudera/Desktop/Deepak/hive_script.hql


Run unix and hadoop command from hive shell:
-------------------------------------------
Unix command-->
hive> !ls -lrt;

Hadoop command -->
hive> dfs -ls /user/hive;

Hive veraibles:
---------------
Two type:
1. hiveconf -- set value locally
	hive> set hiveconf:number=10;
	hive> set hiveconf:number;
	hive> select * from dept where col1=${hiveconf:number};

2. hivevar -- set value globally
	hive> set hivevar:num=10;
	hive> set hivevar:num;
	hivevar:num=10
	hive> select * from dept where col1=${hivevar:num} limit 5;

[cloudera@quickstart Downloads]$ hive --hivevar colname=col1 --hiveconf tblname=db1.dept -e 'select ${hivevar:colname} from ${hiveconf:tblname};'


Hive supported file formatS:
---------------------------
Text Files(csv/tsv)
	1. Does not support block compression
	2. Good write performance but slow read performance.
	3. New fileds can be added to the file but existing cannot be deleted.

Sequence Files:
	1. Each record is stored as ky value pair in binary format
	2. Good write performance then text file
	3. Support block compression.
	4. New fileds can be added to the file but existing cannot be deleted.

Avro Files:
	1. It is a file format plus serialization and deserillization framework.Avro uses json for defining data types and serializes data in binary format.
	2. Average read/write performance.
	3. Support block compression.
	4. Fields can be added,renamed,deleted.

RC files:
	1. Its a columnar file format.Dataset are partitioned both horizontally and vertically.
	2. Faster read performance but compromise write performance.
	3. Provide significant block compression.
	4. New fileds can be added to the file but existing cannot be deleted.

ORC Files:
	1. Optamize row columnar format.Better version of RC file.

Parquet Files:
	1. Faster read and slow write.
	2. Support compression with snappy algorithm.
	3. New fileds can be added to the file but existing cannot be deleted.

Which file format to chose:
	If schema is going to change --> Avro
	Dumping data into HDFS --> Text
	Reading data --> Columnar files
	Storing intermideate data --> sequence files

Modes in Hive:
--------------
hive> set mapred.job.tracker;
mapred.job.tracker=localhost:8021

	1. Embedded Mode: 
		- Driver,metastore service runs on same JVM(Hive service JVM) and uses default derby database to store metadata.Database also runs on same JVM.
		- Only one hive instance can run at a time with this mode.

	2. Local Mode:
		- Driver and metastore service run on same JVM(Hive service JVM) and Database run on seperate JVM.
		- Metastore service connectes with database using JDBC.
		- We can multiple session with this mode.
		- When we have small dataset or hadoop is runnig in psudo mode , we should use this mode.

	3. Remote host mode:
		- Metastore service runs on seperate JVM and not in Hive service JVM.
		- Metastore service connectes with database using JDBC.
		- We should use thsi when hadoop is intalled in distributed mode.

File Compression:
-----------------
Gzip
Bzip2
LZO
snappy

Enable compression - 
	hive> set mapred.compress.map.output=true;
	hive> set mapred.map.output.compression.codec;
	mapred.map.output.compression.codec=org.apache.hadoop.io.compress.DefaultCodec
	hive> set mapred.map.output.compression.codec="";

Set compression for reducer output:
	hive> set mapred.output.compress=true;
	hive> set mapred.output.compression.codec="";


Use Case:
--------

1. Load xml files -- we have to use serde(com.ibm.spss.hive.serde2.xml.XmlSerDe)
		hive> add jar /home/cloudera/Downloads/hivexmlserde-1.0.0.0.jar;
		hive> create table if not exists book_details(title string,author string, country string,company string,price float,year int) row format serde 'com.ibm.spss.hive.serde2.xml.XmlSerDe' with serdeproperties(
"column.xpath.title"="/BOOK/TITLE/text()",
"column.xpath.author"="/BOOK/AUTHOR/text()",
"column.xpath.country"="/BOOK/COUNTRY/text()",
"column.xpath.company"="/BOOK/COMPANY/text()",
"column.xpath.price"="/BOOK/PRICE/text()",
"column.xpath.year"="/BOOK/YEAR/text()"
) stored as inputformat 'com.ibm.spss.hive.serde2.xml.XmlInputFormat'
outputformat 'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat'
tblproperties ("xmlinput.start"="<BOOK>","xmlinput.end"="</BOOK>");

		hive> load data local inpath '/home/cloudera/Downloads/books.xml' into table book_details;
		

Word Count program:
------------------

hive> create table if not exists word_count(name string) stored as textfile;
hive> select word,count(1) as count from (select explode(split(name,',')) as word from word_count) w group by word;


Update and Delete
-----------------
No limitaion for Insert
Limitations only for Update and Delete:

    BEGIN, COMMIT, and ROLLBACK are not yet supported.  All language operations are auto-commit.

    Only ORC file format is supported in this first release.  
  
    Tables must be bucketed to make use of these features.External tables cannot be made ACID tables since the changes on external tables are beyond the control of 	the compactor (HIVE-13175).
    
    Reading/writing to an ACID table from a non-ACID session is not allowed. In other words, the Hive transaction manager must be set to  
    org.apache.hadoop.hive.ql.lockmgr.DbTxnManager in order to work with ACID tables.
    
    LOAD DATA... statement is not supported with transactional tables.  (This was not properly enforced until HIVE-16732)

Set property :
	set hive.support.concurrency=true;
	set hive.enforce.bucketing=true;
	set hive.exec.dynamic.partition.mode=nonstrict;
	set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;
	set hive.compactor.initiator.on=true;
	set hive.compactor.worker.threads=2;
	hive> create table if not exists Hive_ACID (id int , name string , age string) clustered by (id) into 2 buckets  stored as ORC  tblproperties ('transactional' = 'true');
	hive> update Hive_ACID set age='34' where id=1;
	hive> delete from Hive_ACID where id=2;		

Save table data in a file:
--------------------------
	hive> insert overwrite local directory '/home/cloudera/Desktop/Deepak' select * from books1; --> In local 
	hive> insert overwrite directory '/home/cloudera/Desktop/Deepak' select * from books1; --> In HDFS


Read Apache web log file:
-------------------------

CREATE TABLE apachelog (
  host STRING,
  identity STRING,
  user STRING,
  time STRING,
  request STRING,
  status STRING,
  size STRING,
  referer STRING,
  agent STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
  "input.regex" = "([^ ]*) ([^ ]*) ([^ ]*) (-|\\[[^\\]]*\\]) ([^ \"]*|\"[^\"]*\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\"[^\"]*\") ([^ \"]*|\"[^\"]*\"))?")
STORED AS TEXTFILE;

	hive> load data local inpath '/home/cloudera/Desktop/Deepak/ApacheLog' overwrite into table apachelog;

Sample Case study:
-------------------
hive> create table if not exists olympic (athelete STRING,age INT,country STRING,year STRING,closing STRING,sport STRING,gold INT,silver INT,bronze INT,total INT) row format delimited fields terminated by '\t' stored as textfile;

hive> load data local inpath '/home/cloudera/Desktop/Deepak/olympic_data.csv' overwrite into table olympic;

	






